{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branching Image Processor with Aggregating MHA Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fully functioning notebook implements an architecture for processing an image dataset which contains multiple classification tasks.\n",
    "\n",
    "Each batch of images is passed through an ImageToEntities module which creates entitities representing convolutional output features.  The samples in batch are then split by classification task, and passed though a parallel set of MHA Encoders -- one per classification task in the dataset. This is similar to the image-handling approach in [\"Relational Deep Reinforcement Learning\"](https://arxiv.org/abs/1806.01830).\n",
    "\n",
    "Each of these AggregatingMHAEncoders consists of N stacks of MHA/Normalization/Feed-Forward layers, based upon the encoder portion of the encoder/decoder architecture originally described in [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762).  A final layer in each AggregatingMHAEncoder -- either a Max Pooling function, or \"AggegatedMHA\" function -- reduces the dimensionality of each encoders output.  \n",
    "\n",
    "The sub-batches output by the encoders is then re-combined into a single batch for a final module, with each sample in the batch concatenated with it's associated classification task, and this recombined batch is then passed to a final feed-forward layer.\n",
    "\n",
    "For testing, this repository contains one dataset from [\"An Explicitly Relational Neural Network Architecture\"](https://arxiv.org/abs/1905.10307). \n",
    "\n",
    "Each of these architectural elements are shown in more detail in diagrams in the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/branching_mha_encoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "import math\n",
    "import pdb\n",
    "import matplotlib.pylab as plt\n",
    "import fnmatch\n",
    "from  torch.nn.utils import clip_grad_value_\n",
    "import copy\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchingImageProcessor(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, conv_net,encoder_wrappers,recombined_ff,tasks_in_data,d_model):\n",
    "        super(BranchingImageProcessor, self).__init__()\n",
    "        self.conv_net=conv_net\n",
    "        self.d_model=d_model\n",
    "\n",
    "        self.task_encoders = nn.ModuleList(encoder_wrappers[0:])\n",
    "        self.recombined_ff=recombined_ff\n",
    "        self.x_splitter_list=XSplitter.create_x_splitters(tasks_in_data)\n",
    "\n",
    "        \n",
    "    def forward(self, x,task_tensor,y_tensor):\n",
    "        \n",
    "        conv_input=self.conv_net(x)\n",
    "\n",
    "        split_list,split_has_data=XSplitter.split_inputs_by_task(conv_input,task_tensor,y_tensor)\n",
    "        task_encoder_index=0\n",
    "        found_split_with_data=False\n",
    "        for x_split in split_list:\n",
    "            if split_has_data[task_encoder_index]==True:\n",
    "                x_out=self.task_encoders[task_encoder_index](x_split)\n",
    "                if found_split_with_data==False:\n",
    "                    final_input=x_out\n",
    "                    found_split_with_data=True\n",
    "                else:\n",
    "                    final_input=torch.cat((final_input,x_out),0) \n",
    "            task_encoder_index+=1\n",
    "        new_task,new_y_tensor=XSplitter.reconstruct_task_and_y_tensors()\n",
    "\n",
    "        final_output = self.recombined_ff(final_input,new_task)\n",
    "        return final_output,new_y_tensor\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def make_model(cls,args):\n",
    "        \n",
    "        c = copy.deepcopy  \n",
    "        \n",
    "        attn=MultiHeadedAttention(args.encoder_attention_heads,args.d_model)\n",
    "        ffn=EncoderFeedForward(args.d_model, args.encoder_ffn_dim, args.dropout)\n",
    "        encoder_wrappers=[]\n",
    "        \n",
    "        for arg_index in range(args.encoder_count):\n",
    "            encoder = Encoder(EncoderLayer(args.d_model,c(attn),\n",
    "                                            c(ffn),args.dropout),args.encoder_layers)\n",
    "            encoder_wrappers.append(AggregatingMHAEncoder(encoder,args.d_model,args.entity_count,\n",
    "                                 args.aggregate_attention_heads,args.dropout,args.aggregation_method))\n",
    "\n",
    "        recombined_ff= RecombinedFF(args.d_model,args.final_module_hidden_size,args.tgt_class_count,args.entity_count,args.aggregation_method)\n",
    "            \n",
    "        model = BranchingImageProcessor(\n",
    "            ImageToEntities(args),\n",
    "            encoder_wrappers,\n",
    "            recombined_ff,\n",
    "            args.tasks_in_data,\n",
    "            args.d_model)\n",
    "        \n",
    " \n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        return model\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecombinedFF(nn.Module):\n",
    "    def __init__(self, d_model,final_module_hidden_size,tgt_class_count,entity_count,aggregation_method):\n",
    "        super(RecombinedFF, self).__init__()\n",
    "        \n",
    "        if aggregation_method==AGG_METHOD_NONE:\n",
    "            final_module_input_size=(d_model*entity_count)+1\n",
    "        else:\n",
    "            final_module_input_size=d_model+1\n",
    "  \n",
    "        self.proj1 = nn.Linear(final_module_input_size, final_module_hidden_size)\n",
    "        self.proj2 = nn.Linear(final_module_hidden_size,tgt_class_count)\n",
    "\n",
    "    def forward(self,x,task):\n",
    "\n",
    "        task=task.unsqueeze(1)\n",
    "        x=torch.cat((x,task),1)\n",
    "        x1=self.proj1(x)\n",
    "        x2=self.proj2(x1)\n",
    "        return x2      \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSplitter\n",
    "\n",
    "Nice class for splitting and recombining X, Y, and Task values in a dataset.\n",
    "See use in BranchingImageProcessor module above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSplitter:\n",
    "    \n",
    "    # Don't initialize directly.\n",
    "    # Use create_x_splitters factory which creates all XSplitters at once\n",
    "    # and puts them in splitter_list\n",
    "    \n",
    "    x_splitter_list = []\n",
    "    \n",
    "    def __init__(self,select_task):   \n",
    "        self.select_task=select_task\n",
    "        \n",
    "    # create sub-batch of x,y, and task tensors \n",
    "    # for this XSplitters's select_task\n",
    "    \n",
    "    def select(self,x,task_tensor,y_tensor):\n",
    "        mask=[self.select_task==task_tensor]\n",
    "        split=x[mask]\n",
    "        self.task_split=task_tensor[mask]\n",
    "        self.y_split=y_tensor[mask]\n",
    "        if len(split)==0:\n",
    "            return(split,False)\n",
    "        else:\n",
    "            return(split,True)\n",
    "        \n",
    "    def get_task_and_y_splits(self):\n",
    "        return(self.task_split,self.y_split)\n",
    "    \n",
    "    # Pass list of tasks in dataset to create list of XSplitter objects.\n",
    "    \n",
    "    @classmethod\n",
    "    def create_x_splitters(cls,task_list):\n",
    "        for task in task_list:\n",
    "            cls.x_splitter_list.append(XSplitter(int(task)))\n",
    "            \n",
    "    # Pass the batch x, task, and y values.\n",
    "\n",
    "    @classmethod\n",
    "    def split_inputs_by_task(cls,x,task_tensor,y_tensor):\n",
    "        split_list=[]\n",
    "        split_has_data=[]\n",
    "        for x_splitter in cls.x_splitter_list:\n",
    "            split,has_data=x_splitter.select(x,task_tensor,y_tensor)\n",
    "            split_list.append(split)\n",
    "            split_has_data.append(has_data)\n",
    "        return (split_list,split_has_data)  \n",
    "\n",
    "    # after parallel encoders are run, output x values will be grouped into contiguous blocks by task.\n",
    "    # so here we reorganize y and task values into contiguous blocks as well.  See diagram at top of notebook.\n",
    "    @classmethod\n",
    "    def reconstruct_task_and_y_tensors(cls):\n",
    "        first_split_with_data=False\n",
    "        for x_splitter in cls.x_splitter_list:\n",
    "            task_split,y_split=x_splitter.get_task_and_y_splits()\n",
    "            if len(task_split)>0:\n",
    "                if first_split_with_data==False:\n",
    "                    new_task_tensor=task_split\n",
    "                    new_y_tensor=y_split\n",
    "                    first_split_with_data=True\n",
    "                else:\n",
    "                    new_task_tensor=torch.cat((new_task_tensor,task_split),0)\n",
    "                    new_y_tensor=torch.cat((new_y_tensor,y_split),0)\n",
    "        return (new_task_tensor,new_y_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following module has an identical structure to the encoder half of the Transformer Architecture as initially presented in  [\"Attention is All You Need\"](https://arxiv.org/abs/1806.01830) https://arxiv.org/abs/1706.03762.\n",
    "The difference is that the second dimension of the final output is reduced from args.entity_count to one, either via a max function, or via an AggregatingMultiHeadedAttention function.\n",
    "\n",
    "$Aggregating Multi-Headed Encoder$\n",
    "\n",
    "<img src=\"images/encoder_wrapper.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatingMHAEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, d_model,entity_count,heads,dropout,aggregation_method):\n",
    "        super(AggregatingMHAEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.aggregating_multi_head_attn=AggregatingMultiHeadedAttention(heads,d_model,entity_count,dropout)\n",
    "        self.aggregation_method=aggregation_method\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        if self.aggregation_method==AGG_METHOD_NONE:\n",
    "            x=x.view(x.shape[0],-1)\n",
    "            return x\n",
    "        if self.aggregation_method==AGG_METHOD_MAX:\n",
    "            highest_entity=torch.max(x,1,keepdim=True)\n",
    "            x=highest_entity[0]\n",
    "            x=x.squeeze()\n",
    "        else:\n",
    "            x=self.aggregating_multi_head_attn(x,x,x)\n",
    "        return F.relu(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "The code comprising the Encoder layer was created by modifying code from the elegant Pytorch implementation in [\"The Annotated Transformer\"](http://nlp.seas.harvard.edu/2018/04/03/attention.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer,N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Pass the input through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def attention(query, key, value, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))/ math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        \"Implements Figure 2\"\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value =             [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "\n",
    "class EncoderFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(EncoderFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "    @classmethod   \n",
    "    def get_std_opt(cls,model):\n",
    "        return NoamOpt(model.d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [\"Relational Deep Reinforcement Learning\"](https://arxiv.org/abs/1806.01830), which uses a similar image transformation module to produce entities for an MHA function, the MHA layers are followed by a max pooling function which reduces the dimensionality to a single entity.   \n",
    "In my model, as a final step to the AggregatingMHAEncoder, I offer the max pooling if the use_max argument is true, otherwise I use the AggregatingMultiHeadedAttention function below. \n",
    "This function is identical to the MHA layer in the encoder module, except that here the entities are flattened to form the initial query, which results in a final output which is aggregated to a single entity per-batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/top.png\"  align=\"center\"/>\n",
    "<img src=\"images/attention.png\"  width=\"200\" height=\"80\" align=\"center\"/>\n",
    "<img src=\"images/aggregated_attention.png\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatingMultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, entity_count,dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(AggregatingMultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.key_value_linears = clones(nn.Linear(d_model, d_model), 2)\n",
    "        self.query_linear = nn.Linear(d_model*entity_count,d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        key, value =             [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.key_value_linears, (key, value))]\n",
    "        \n",
    "        flat_query=query.view(nbatches,-1)\n",
    "\n",
    "        query = self.query_linear(flat_query).view(nbatches,-1,self.h,self.d_k).transpose(1,2)\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, self.h * self.d_k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageToEntities Module\n",
    "\n",
    "Module consists of:\n",
    " - Convolutional Layer\n",
    " - Layer which adds channels representing the x and y coordinates  of the convolutional output features, as described in [\"An Intriguing Failing of Convolutional Neural Networks\"](https://arxiv.org/pdf/1807.03247.pdf)   \n",
    " using implementation borrowed from [\"CoordConv-Pytorch\"](https://github.com/mkocabas/CoordConv-pytorch/blob/master/CoordConv.py).\n",
    " - A reshape/permutation step which creates a set of entities for MultiHeadedAttention processing.  This transformation and depiction below are similar to [\"Relational Deep Reinforcement Learning\"](https://arxiv.org/abs/1806.01830).\n",
    "\n",
    "\n",
    "<img src=\"images/image_to_entities.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToEntities(nn.Module):\n",
    "    def __init__(self,args,):\n",
    "        super(ImageToEntities, self).__init__()\n",
    "        self.x_dim=args.x_dim\n",
    "        self.y_dim=args.y_dim\n",
    "        self.out_channels=args.out_channels\n",
    "        self.in_channels=args.in_channels\n",
    "        self.add_output_coords=args.add_output_coords\n",
    "        self.dropout = nn.Dropout(args.conv_dropout)\n",
    "        \n",
    "        if self.add_output_coords:\n",
    "            self.out_channels-=2\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=12, stride=6, padding=0)\n",
    "        \n",
    "        if self.add_output_coords:\n",
    "            #could make this dynamic rather than hard-coding 5 by passing tmp value to convnet and testing len of features\n",
    "            self.coord_output_adder = AddCoordsTh(x_dim=5, y_dim=5, with_r=args.with_r)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.add_output_coords:\n",
    "            x = self.coord_output_adder(x)\n",
    "        x1=self.make_entities(x)\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = F.relu(x1)\n",
    "        return x2\n",
    " \n",
    "    \n",
    "    def make_entities(self,x):\n",
    "        x=x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        x=x.permute(0,2,1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AddCoordsTh(nn.Module):\n",
    "    def __init__(self, x_dim=64, y_dim=64, with_r=False):\n",
    "        super(AddCoordsTh, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor: (batch, c, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size_tensor = input_tensor.shape[0]\n",
    "\n",
    "        xx_ones = torch.ones([1, self.y_dim], dtype=torch.float32)\n",
    "        xx_ones = xx_ones.unsqueeze(-1)\n",
    "\n",
    "        xx_range = torch.arange(self.x_dim, dtype=torch.float32).unsqueeze(0)\n",
    "        xx_range = xx_range.unsqueeze(1)\n",
    "\n",
    "        xx_channel = torch.matmul(xx_ones, xx_range)\n",
    "        xx_channel = xx_channel.unsqueeze(-1)\n",
    "\n",
    "        yy_ones = torch.ones([1, self.x_dim], dtype=torch.float32)\n",
    "        yy_ones = yy_ones.unsqueeze(1)\n",
    "\n",
    "        yy_range = torch.arange(self.y_dim, dtype=torch.float32).unsqueeze(0)\n",
    "        yy_range = yy_range.unsqueeze(-1)\n",
    "\n",
    "        yy_channel = torch.matmul(yy_range, yy_ones)\n",
    "        yy_channel = yy_channel.unsqueeze(-1)\n",
    "        \n",
    "        xx_channel = xx_channel.permute(0, 3, 2, 1)\n",
    "        yy_channel = yy_channel.permute(0, 3, 2, 1)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (self.x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (self.y_dim - 1)\n",
    "\n",
    "        #xx_channel = xx_channel * 2 - 1\n",
    "        #yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size_tensor, 1, 1, 1)\n",
    "        yy_channel = yy_channel.repeat(batch_size_tensor, 1, 1, 1)\n",
    "\n",
    "        ret = torch.cat([input_tensor, xx_channel, yy_channel], dim=1)\n",
    "        \n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "\n",
    "Using argparse, to ease port to command-line version.   See hack at end of main to make this work with Jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG_METHOD_NONE=0\n",
    "AGG_METHOD_MAX=1\n",
    "AGG_METHOD_MHA=2\n",
    "\n",
    "class Arguments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser(description='train')\n",
    "        self.add_arguments()\n",
    "        self.args = self.parser.parse_args()\n",
    "\n",
    "    def add_arguments(self): \n",
    "        self.add_conv_arguments()\n",
    "        self.add_dataset_arguments()\n",
    "        self.add_training_arguments()\n",
    "        self.add_encoder_arguments()\n",
    "        \n",
    "    def add_encoder_arguments(self):\n",
    "        # Normally encoder-count equals number of tasks in data \n",
    "        # But could be extra dormant ones to maintain model architecture across model save/restore\n",
    "        # with different datasets\n",
    "        self.parser.add_argument('--encoder-count', type=int,metavar='N',\n",
    "                                 default=3,help='number of parallel encoders')\n",
    "        self.parser.add_argument('--tasks-in-data', nargs='+', default=['0','1','2'])     \n",
    "        self.parser.add_argument('--entity_count', default=25, type=int, metavar='N',\n",
    "            help='number of entities')\n",
    "        self.parser.add_argument('--aggregation-method',type=int, metavar='N',\n",
    "                            default=AGG_METHOD_NONE,help='aggregation method for encoder output')\n",
    "        self.parser.add_argument('--dropout', type=float, metavar='D',\n",
    "                            default=0.1,help='dropout probability')    \n",
    "        self.parser.add_argument('--d-model', type=int, metavar='N',\n",
    "                            default=32,help='encoder d_model')\n",
    "        self.parser.add_argument('--encoder-layers', type=int, metavar='N',\n",
    "                            default=3,help='num encoder layers')\n",
    "        self.parser.add_argument('--encoder-attention-heads', type=int, metavar='N',\n",
    "                            default=16,help='num encoder attention heads')\n",
    "        self.parser.add_argument('--attention-dropout', type=float, metavar='D',\n",
    "                            default=0.1,help='dropout probability for attention weights')\n",
    "        self.parser.add_argument('--encoder-ffn-dim', type=int, metavar='N',\n",
    "                            default=256,help='encoder dimension for FFN') \n",
    "        self.parser.add_argument('--tgt_class_count', type=int, metavar='N',\n",
    "                            default=2,help='tgt class count') \n",
    "        self.parser.add_argument('--final-module-hidden-size', type=int, metavar='N',\n",
    "                            default=1024,help='generator_hidden_size')                \n",
    "        self.parser.add_argument('--aggregate-attention-heads', type=int, metavar='N',\n",
    "                            default=8,help='number of heads for AggregatingMHAAttention step')\n",
    "      \n",
    "                        \n",
    "    def add_training_arguments(self):\n",
    "        self.parser.add_argument('--do-restore',default=False,action='store_true',help=\"do restore\")\n",
    "        self.parser.add_argument('--do-train',default=True,action='store_false',help=\"do train\") \n",
    "        self.parser.add_argument('--freeze-lower-layers',default=False,action='store_true',help=\"freeze lower layers\")\n",
    "        self.parser.add_argument('--check-interval', type=int, metavar='N',\n",
    "                                 default=100,help='check interval')\n",
    "        self.parser.add_argument('--base-path', metavar='N',\n",
    "                                 default=\"/Users/azulay/jupyter_work/\",help='base path') \n",
    "        self.parser.add_argument('--model-path', metavar='N',\n",
    "                                 default=\"model_data/saved_parms\",help='model path') \n",
    "        self.parser.add_argument('--tensorboard-path', metavar='N',\n",
    "                                 default=\"rt_logs\",help='tensorflow path')         \n",
    "        self.parser.add_argument('--batch-size', type=int, metavar='N',\n",
    "                                 default=100,help='batch size') \n",
    "        self.parser.add_argument('--test-batch-size', type=int, metavar='N',\n",
    "                                 default=200,help='batch size') \n",
    "        self.parser.add_argument('--max-epochs', type=int, metavar='N',\n",
    "                                 default=20,help='max epochs')\n",
    "        self.parser.add_argument('--default-device', metavar='N',\n",
    "                                 default=\"cpu\",help='default device')     \n",
    "        self.parser.add_argument('--ignore-saved-counts',default=True,action='store_false',help=\"do train\") \n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_dataset_arguments(self):\n",
    "        self.parser.add_argument('--data-dir', metavar='N',\n",
    "                                 default=\"datasets/3task_col_patts_pentos/\",help='data dir')      \n",
    "        self.parser.add_argument('--test-dir', metavar='N',\n",
    "                                 default=\"datasets/3task_col_patts_stripes/\",help='valid data dir')      \n",
    "        self.parser.add_argument('--report-size', type=int, metavar='N',\n",
    "                                 default=2,help='report size')  \n",
    "        self.parser.add_argument('--report-label-offset', type=int, metavar='N',\n",
    "                                 default=0,help='report label offset') \n",
    "        self.parser.add_argument('--report-task-offset', type=int, metavar='N',\n",
    "                                 default=1,help='report task offset') \n",
    "\n",
    "        \n",
    "    def add_conv_arguments(self):\n",
    "        self.parser.add_argument('--y-dim', type=int, metavar='N',\n",
    "                                 default=36,help='size of y_dimention')\n",
    "        self.parser.add_argument('--x-dim', type=int, metavar='N',\n",
    "                                 default=36,help='size of x_dimention')\n",
    "        self.parser.add_argument('--in-channels', type=int, metavar='N',\n",
    "                                 default=3,help='# input channels')\n",
    "        self.parser.add_argument('--out-channels', type=int, metavar='N',\n",
    "                                 default=32,help='# output channels')\n",
    "        self.parser.add_argument('--kernel-size', type=int, metavar='N',\n",
    "                                 default=12,help='kernel size')\n",
    "        self.parser.add_argument('--stride', type=int, metavar='N',\n",
    "                                 default=6,help='stride')\n",
    "        self.parser.add_argument('--add-output-coords', action='store_false',\n",
    "                                 default=True,help='add X and Y coordinate channels')\n",
    "        self.parser.add_argument('--with-r', action='store_false',\n",
    "                                 default=False,help='with r')\n",
    "        self.parser.add_argument('--conv-dropout', type=float, metavar='D',\n",
    "                            default=0.0,help='dropout probability')    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \n",
    "    def __init__(self,stack,stack_optimizer,criterion,args):\n",
    "        self.stack=stack\n",
    "        self.stack_optimizer=stack_optimizer\n",
    "        self.criterion=criterion\n",
    "        self.args=args\n",
    " \n",
    "    def train(self,input_tensor, y_tensor,task_tensor): \n",
    "        \n",
    "        if args.freeze_lower_layers:\n",
    "            self.stack.eval()\n",
    "            self.stack.generator.train()\n",
    "            for task_encoder in self.stack.task_encoders:\n",
    "                task_encoder.train()\n",
    "        else:\n",
    "            self.stack.train()\n",
    "                \n",
    "        self.stack_optimizer.optimizer.zero_grad()    \n",
    "\n",
    "        stack_output,y_tensor=self.stack(input_tensor,task_tensor,y_tensor)\n",
    "\n",
    "        loss = self.criterion(stack_output, y_tensor)\n",
    "        loss.backward()\n",
    " \n",
    "        self.stack_optimizer.step()\n",
    "        _, predicted = torch.max(stack_output, 1)\n",
    "        correct = (predicted == y_tensor).sum()\n",
    "        accuracy = 100 * correct / y_tensor.size(0)\n",
    "        return loss,accuracy\n",
    "\n",
    "    \n",
    "    def eval(self,input_tensor,y_tensor,task_tensor):\n",
    "        self.stack.eval()\n",
    "        stack_output,y_tensor=self.stack(input_tensor,task_tensor,y_tensor)\n",
    "        loss = self.criterion(stack_output, y_tensor)\n",
    "        _, predicted = torch.max(stack_output, 1)\n",
    "        correct = (predicted == y_tensor).sum()\n",
    "        accuracy = 100 * correct / y_tensor.size(0)\n",
    "        return loss,accuracy\n",
    "    \n",
    "    def restore(self,model_dir):\n",
    "        checkpoint = torch.load(model_dir)\n",
    "        self.stack.load_state_dict(checkpoint['stack_state_dict'])\n",
    "        self.stack_optimizer.optimizer.load_state_dict(checkpoint['stack_optimizer_state_dict'])\n",
    "        lowest_mean_loss=checkpoint['loss']\n",
    "        lowest_test_loss=checkpoint['test_loss']\n",
    "        epoch=checkpoint['epoch']\n",
    "        batch_count=checkpoint['batch_count']\n",
    "        print(\"restoring\")\n",
    "        print(\"saved epoch was \",epoch,\"batch count was\",batch_count)\n",
    "        return lowest_mean_loss,lowest_test_loss,epoch,batch_count\n",
    "        \n",
    "\n",
    "    def save(self,mean_loss,test_loss,epoch,batch_count,model_dir):\n",
    "        torch.save({\n",
    "            'stack_state_dict': self.stack.state_dict(),\n",
    "            'stack_optimizer_state_dict': self.stack_optimizer.optimizer.state_dict(),                         \n",
    "            'loss': mean_loss,\n",
    "            'test_loss' : test_loss,\n",
    "            'epoch' : epoch,\n",
    "            'batch_count' : batch_count,\n",
    "             }, model_dir)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/azulay/opt/anaconda3/envs/new/lib/python3.6/site-packages/ipykernel_launcher.py\n",
      "Namespace(add_output_coords=True, aggregate_attention_heads=8, aggregation_method=0, attention_dropout=0.1, base_path='/Users/azulay/jupyter_work/', batch_size=100, check_interval=100, conv_dropout=0.0, d_model=32, data_dir='datasets/3task_col_patts_pentos/', default_device='cpu', do_restore=False, do_train=True, dropout=0.1, encoder_attention_heads=16, encoder_count=3, encoder_ffn_dim=256, encoder_layers=3, entity_count=25, final_module_hidden_size=1024, freeze_lower_layers=False, ignore_saved_counts=True, in_channels=3, kernel_size=12, max_epochs=20, model_path='model_data/saved_parms', out_channels=32, report_label_offset=0, report_size=2, report_task_offset=1, stride=6, tasks_in_data=['0', '1', '2'], tensorboard_path='rt_logs', test_batch_size=200, test_dir='datasets/3task_col_patts_stripes/', tgt_class_count=2, with_r=False, x_dim=36, y_dim=36)\n",
      "got it /Users/azulay/jupyter_work/external_code\n",
      "batches per epoch: 2500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cf14bcfd660a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cf14bcfd660a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                     shuffle=True,num_workers=1)\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mbatch_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/new/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "\n",
    "    tensorboard_path=os.path.join(args.base_path,args.tensorboard_path)\n",
    "    data_dir=os.path.join(args.base_path,args.data_dir)\n",
    "    test_dir=os.path.join(args.base_path,args.test_dir) \n",
    "    model_path=os.path.join(args.base_path,args.model_path)\n",
    "    external_code_path=os.path.join(args.base_path,\"external_code\")\n",
    "    \n",
    "    # Dataset has to live outside this notebook in order to use set_start_method('spawn') in main\n",
    "    # Otherwise notebook hangs when using GPU.  Not sure why.  Will investigate at some point.\n",
    "    if external_code_path not in sys.path:\n",
    "        sys.path.append(external_code_path)\n",
    "        \n",
    "    from multi_task_dataset import MultiTaskDataset\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"thing grid\",log_dir=tensorboard_path)\n",
    "\n",
    "    lowest_mean_loss=999999\n",
    "    lowest_test_loss=999999\n",
    "    total_loss=0   \n",
    "    total_accuracy=0\n",
    "    epoch=0 \n",
    "    batch_count=0\n",
    "    check_count=0\n",
    "    data_files = os.listdir(data_dir) \n",
    "    \n",
    "    total_data_files = len(data_files)\n",
    "    batches_per_epoch=total_data_files//args.batch_size\n",
    "    print(\"batches per epoch:\",batches_per_epoch)\n",
    "\n",
    "    if args.default_device==\"cuda\":\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    stack=BranchingImageProcessor.make_model(args)\n",
    "\n",
    "    if args.default_device==\"cuda\":\n",
    "        stack=stack.cuda()\n",
    "    \n",
    "    stack_optimizer=NoamOpt.get_std_opt(stack)\n",
    "    criterion = nn.CrossEntropyLoss()   \n",
    "\n",
    "    train=Train(stack,stack_optimizer,criterion,args)\n",
    "    \n",
    "    if args.do_train==False:\n",
    "        lowest_mean_loss,lowest_test_loss,epoch=train.restore(model_path)\n",
    "        test_dataset = MultiTaskDataset(test_dir)\n",
    "            \n",
    "        test_dataloader = data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n",
    "                                shuffle=True,num_workers=1)    \n",
    "        \n",
    "        for v_batch, sample_v_batched in enumerate(test_dataloader):\n",
    "            v_image = sample_v_batched['image'].to(torch.device(args.default_device))\n",
    "            v_label = sample_v_batched['label'].to(torch.device(args.default_device)) \n",
    "            v_task = sample_v_batched['task'].to(torch.device(args.default_device))\n",
    "            test_loss,test_accuracy=train.eval(v_image,v_label,v_task)\n",
    "            print(\"Test loss\",test_loss.item(),\"test accuracy\",test_accuracy.item())\n",
    "        \n",
    "    else:\n",
    "\n",
    "        if args.do_restore:\n",
    "            lowest_mean_loss,lowest_test_loss,epoch,batch_count=train.restore(model_path)\n",
    "            if args.ignore_saved_counts==True:\n",
    "                print(\"zeroing saved counts\")\n",
    "                lowest_mean_loss=999999\n",
    "                lowest_test_loss=999999\n",
    "                epoch=0 \n",
    "                batch_count=0\n",
    "\n",
    "        if args.freeze_lower_layers:\n",
    "            print(\"!!***** FREEZING PARMS *********!!\")\n",
    "            for param in stack.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in stack.generator.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for epoch in range(epoch,args.max_epochs):\n",
    "\n",
    "            train_dataset = MultiTaskDataset(args,data_dir)\n",
    "            test_dataset = MultiTaskDataset(args,test_dir)\n",
    "\n",
    "            dataloader = data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                    shuffle=True,num_workers=1)\n",
    "\n",
    "            test_dataloader = data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n",
    "                                    shuffle=True,num_workers=1)\n",
    " \n",
    "            for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "                batch_count+=1\n",
    "                image = sample_batched['image'].to(torch.device(args.default_device))\n",
    "                label = sample_batched['label'].to(torch.device(args.default_device))\n",
    "                task = sample_batched['task'].to(torch.device(args.default_device))\n",
    "\n",
    "                loss,accuracy=train.train(image,label,task)\n",
    "                check_count+=1\n",
    "                total_loss+=loss.item()\n",
    "                total_accuracy+=accuracy.item()\n",
    "                \n",
    "                if batch_count % args.check_interval==0:\n",
    "                    mean_loss=total_loss/check_count\n",
    "                    mean_accuracy=total_accuracy//check_count\n",
    "                    writer.add_scalar(\"loss\", mean_loss, batch_count)     \n",
    "                    writer.add_scalar(\"accuracy\",accuracy,batch_count)     \n",
    "                    print(\"epoch\",epoch,\"count \",batch_count,\"loss\",mean_loss, \"lowest loss\",lowest_mean_loss,\"accuracy\",mean_accuracy)\n",
    "                    sample_v_batched = iter(test_dataloader).next()\n",
    "                    v_image = sample_v_batched['image'].to(torch.device(args.default_device))\n",
    "                    v_label = sample_v_batched['label'].to(torch.device(args.default_device)) \n",
    "                    v_task = sample_v_batched['task'].to(torch.device(args.default_device))\n",
    "                    test_loss,test_accuracy=train.eval(v_image,v_label,v_task)\n",
    "                    test_loss=test_loss.item()\n",
    "                    writer.add_scalar(\"test loss\", test_loss, batch_count)\n",
    "                    writer.add_scalar(\"test accuracy\",test_accuracy,batch_count)\n",
    "                    print(\"epoch\",epoch,\"test loss\",test_loss, \"lowest test loss\",lowest_test_loss,\"test accuracy\",test_accuracy)\n",
    "                    if test_loss<lowest_test_loss:\n",
    "                        lowest_test_loss=test_loss\n",
    "                        train.save(lowest_mean_loss,lowest_test_loss,epoch,batch_count,model_path)\n",
    "                    if mean_loss<lowest_mean_loss:\n",
    "                        lowest_mean_loss=mean_loss\n",
    "                    total_loss=0\n",
    "                    total_accuracy=0\n",
    "                    check_count=0\n",
    "                if batch_count % batches_per_epoch ==0:\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    try:\n",
    "        set_start_method('spawn',force=True)\n",
    "    except RuntimeError:\n",
    "        print(\"bad\")\n",
    "\n",
    "    \n",
    "    print(sys.argv[0])\n",
    "    sys.argv=[sys.argv[0]]\n",
    "    arguments=Arguments()\n",
    "    args=arguments.args\n",
    "    print(args)\n",
    "    main(args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
