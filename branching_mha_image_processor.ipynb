{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branching Image Processor with Aggregating MHA Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fully functioning notebook implements an architecture for processing an image dataset which contains multiple classification tasks.\n",
    "\n",
    "Each batch of images is passed through an ImageToEntities module which creates entitities representing convolutional output features.  The samples in batch are then split by classification task, and passed though a parallel set of MHA Encoders -- one per classification task in the dataset. This is similar to the image-handling approach in [\"Relational Deep Reinforcement Learning\"](https://arxiv.org/abs/1806.01830).\n",
    "\n",
    "Each of these AggregatingMHAEncoders consists of N stacks of MHA/Normalization/Feed-Forward layers, based upon the encoder portion of the encoder/decoder architecture originally described in [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762).  A final layer in each AggregatingMHAEncoder -- either a Max Pooling function, or \"AggegatedMHA\" function -- reduces the dimensionality of each encoders output.  \n",
    "\n",
    "The sub-batches output by the encoders is then re-combined into a single batch for a final module, with each sample in the batch concatenated with it's associated classification task, and this recombined batch is then passed to a final feed-forward layer.\n",
    "\n",
    "For testing, this repository contains one dataset from [\"An Explicitly Relational Neural Network Architecture\"](https://arxiv.org/abs/1905.10307). \n",
    "\n",
    "Each of these architectural elements are shown in more detail in diagrams in the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/branching_mha_encoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "import math\n",
    "import pdb\n",
    "import matplotlib.pylab as plt\n",
    "import fnmatch\n",
    "from  torch.nn.utils import clip_grad_value_\n",
    "import copy\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchingImageProcessor(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_net,encoder_wrappers,recombined_ff,tasks_in_data,d_model):\n",
    "        super(BranchingImageProcessor, self).__init__()\n",
    "        self.conv_net=conv_net\n",
    "        self.d_model=d_model\n",
    "\n",
    "        self.task_encoders = nn.ModuleList(encoder_wrappers[0:])\n",
    "        self.recombined_ff=recombined_ff\n",
    "        self.x_splitter_list=XSplitter.create_x_splitters(tasks_in_data)\n",
    "\n",
    "        \n",
    "    def forward(self, x,task_tensor,y_tensor):\n",
    "        \n",
    "        conv_input=self.conv_net(x)\n",
    "\n",
    "        split_list,split_has_data=XSplitter.split_inputs_by_task(conv_input,task_tensor,y_tensor)\n",
    "        task_encoder_index=0\n",
    "        found_split_with_data=False\n",
    "        for x_split in split_list:\n",
    "            if split_has_data[task_encoder_index]==True:\n",
    "                x_out=self.task_encoders[task_encoder_index](x_split)\n",
    "                if found_split_with_data==False:\n",
    "                    final_input=x_out\n",
    "                    found_split_with_data=True\n",
    "                else:\n",
    "                    final_input=torch.cat((final_input,x_out),0) \n",
    "            task_encoder_index+=1\n",
    "        new_task,new_y_tensor=XSplitter.reconstruct_task_and_y_tensors()\n",
    "\n",
    "        final_output = self.recombined_ff(final_input,new_task)\n",
    "        return final_output,new_y_tensor\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def make_model(cls,args):\n",
    "        \n",
    "        c = copy.deepcopy  \n",
    "        \n",
    "        attn=MultiHeadedAttention(args.encoder_attention_heads,args.d_model)\n",
    "        ffn=EncoderFeedForward(args.d_model, args.encoder_ffn_dim, args.dropout)\n",
    "        encoder_wrappers=[]\n",
    "        \n",
    "        for arg_index in range(args.encoder_count):\n",
    "            encoder = Encoder(EncoderLayer(args.d_model,c(attn),\n",
    "                                            c(ffn),args.dropout),args.encoder_layers)\n",
    "            encoder_wrappers.append(AggregatingMHAEncoder(encoder,args.d_model,args.entity_count,\n",
    "                                 args.aggregate_attention_heads,args.dropout,args.use_max))\n",
    "\n",
    "        recombined_ff= RecombinedFF(args.d_model,args.final_module_hidden_size,args.tgt_class_count)\n",
    "            \n",
    "        model = BranchingImageProcessor(\n",
    "            ImageToEntities(args),\n",
    "            encoder_wrappers,\n",
    "            recombined_ff,\n",
    "            args.tasks_in_data,\n",
    "            args.d_model)\n",
    "        \n",
    " \n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        return model\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecombinedFF(nn.Module):\n",
    "    def __init__(self, d_model,final_module_hidden_size,tgt_class_count):\n",
    "        super(RecombinedFF, self).__init__()\n",
    "  \n",
    "        self.proj1 = nn.Linear(d_model+1, final_module_hidden_size)\n",
    "        self.proj2 = nn.Linear(final_module_hidden_size,tgt_class_count)\n",
    "\n",
    "    def forward(self,x,task):\n",
    "\n",
    "        task=task.unsqueeze(1)\n",
    "        x=torch.cat((x,task),1)\n",
    "        x1=self.proj1(x)\n",
    "        x2=self.proj2(x1)\n",
    "        return x2      \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XSplitter\n",
    "\n",
    "Nice class for splitting and recombining X, Y, and Task values in a dataset.\n",
    "See use in BranchingImageProcessor module above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XSplitter:\n",
    "    \n",
    "    # Don't initialize directly.\n",
    "    # Use create_x_splitters factory which creates all XSplitters at once\n",
    "    # and puts them in splitter_list\n",
    "    \n",
    "    x_splitter_list = []\n",
    "    \n",
    "    def __init__(self,select_task):   \n",
    "        self.select_task=select_task\n",
    "        \n",
    "    # create sub-batch of x,y, and task tensors \n",
    "    # for this XSplitters's select_task\n",
    "    \n",
    "    def select(self,x,task_tensor,y_tensor):\n",
    "        mask=[self.select_task==task_tensor]\n",
    "        split=x[mask]\n",
    "        self.task_split=task_tensor[mask]\n",
    "        self.y_split=y_tensor[mask]\n",
    "        if len(split)==0:\n",
    "            return(split,False)\n",
    "        else:\n",
    "            return(split,True)\n",
    "        \n",
    "    def get_task_and_y_splits(self):\n",
    "        return(self.task_split,self.y_split)\n",
    "    \n",
    "    # Pass list of tasks in dataset to create list of XSplitter objects.\n",
    "    \n",
    "    @classmethod\n",
    "    def create_x_splitters(cls,task_list):\n",
    "        for task in task_list:\n",
    "            cls.x_splitter_list.append(XSplitter(int(task)))\n",
    "            \n",
    "    # Pass the batch x, task, and y values.\n",
    "\n",
    "    @classmethod\n",
    "    def split_inputs_by_task(cls,x,task_tensor,y_tensor):\n",
    "        split_list=[]\n",
    "        split_has_data=[]\n",
    "        for x_splitter in cls.x_splitter_list:\n",
    "            split,has_data=x_splitter.select(x,task_tensor,y_tensor)\n",
    "            split_list.append(split)\n",
    "            split_has_data.append(has_data)\n",
    "        return (split_list,split_has_data)  \n",
    "\n",
    "    # after parallel encoders are run, output x values will be grouped into contiguous blocks by task.\n",
    "    # so here we reorganize y and task values into contiguous blocks as well.  See diagram at top of notebook.\n",
    "    @classmethod\n",
    "    def reconstruct_task_and_y_tensors(cls):\n",
    "        first_split_with_data=False\n",
    "        for x_splitter in cls.x_splitter_list:\n",
    "            task_split,y_split=x_splitter.get_task_and_y_splits()\n",
    "            if len(task_split)>0:\n",
    "                if first_split_with_data==False:\n",
    "                    new_task_tensor=task_split\n",
    "                    new_y_tensor=y_split\n",
    "                    first_split_with_data=True\n",
    "                else:\n",
    "                    new_task_tensor=torch.cat((new_task_tensor,task_split),0)\n",
    "                    new_y_tensor=torch.cat((new_y_tensor,y_split),0)\n",
    "        return (new_task_tensor,new_y_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following module has an identical structure to the encoder half of the Transformer Architecture as initially presented in  [\"Attention is All You Need\"](https://arxiv.org/abs/1806.01830) https://arxiv.org/abs/1706.03762.\n",
    "The difference is that the second dimension of the final output is reduced from args.entity_count to one, either via a max function, or via an AggregatingMultiHeadedAttention function.\n",
    "\n",
    "$Aggregating Multi-Headed Encoder$\n",
    "\n",
    "<img src=\"images/encoder_wrapper.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatingMHAEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, d_model,entity_count,heads,dropout,use_max):\n",
    "        super(AggregatingMHAEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.aggregating_multi_head_attn=AggregatingMultiHeadedAttention(heads,d_model,entity_count,dropout)\n",
    "        self.use_max=use_max\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.encoder(x)\n",
    "        if self.use_max==True:\n",
    "            highest_entity=torch.max(x,1,keepdim=True)\n",
    "            x=highest_entity[0]\n",
    "            x=x.squeeze()\n",
    "        else:\n",
    "            x=self.aggregating_multi_head_attn(x,x,x)\n",
    "        return F.relu(x) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "The code comprising the Encoder layer was created by modifying code from the elegant Pytorch implementation in [\"The Annotated Transformer\"](http://nlp.seas.harvard.edu/2018/04/03/attention.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer,N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Pass the input through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def attention(query, key, value, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))/ math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        \"Implements Figure 2\"\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value =             [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        #pdb.set_trace()\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "\n",
    "class EncoderFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(EncoderFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "    @classmethod   \n",
    "    def get_std_opt(cls,model):\n",
    "        return NoamOpt(model.d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [\"Relational Deep Reinforcement Learning\"](https://arxiv.org/abs/1806.01830), which uses a similar image transformation module to produce entities for an MHA function, the MHA layers are followed by a max pooling function which reduces the dimensionality to a single entity.   \n",
    "In my model, as a final step to the AggregatingMHAEncoder, I offer the max pooling if the use_max argument is true, otherwise I use the AggregatingMultiHeadedAttention function below. \n",
    "This function is identical to the MHA layer in the encoder module, except that here the entities are flattened to form the initial query, which results in a final output which is aggregated to a single entity per-batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/top.png\"  align=\"center\"/>\n",
    "<img src=\"images/attention.png\"  width=\"200\" height=\"80\" align=\"center\"/>\n",
    "<img src=\"images/aggregated_attention.png\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatingMultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, entity_count,dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(AggregatingMultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.key_value_linears = clones(nn.Linear(d_model, d_model), 2)\n",
    "        self.query_linear = nn.Linear(d_model*entity_count,d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        key, value =             [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.key_value_linears, (key, value))]\n",
    "        \n",
    "        flat_query=query.view(nbatches,-1)\n",
    "\n",
    "        query = self.query_linear(flat_query).view(nbatches,-1,self.h,self.d_k).transpose(1,2)\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, self.h * self.d_k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageToEntities Module\n",
    "\n",
    "Module consists of:\n",
    " - Convolutional Layer\n",
    " - Layer which adds channels representing the x and y coordinates  of the convolutional output features, as described in [\"An Intriguing Failing of Convolutional Neural Networks\"](https://arxiv.org/pdf/1807.03247.pdf)   \n",
    " using implementation borrowed from [\"CoordConv-Pytorch\"](https://github.com/mkocabas/CoordConv-pytorch/blob/master/CoordConv.py).\n",
    " - A reshape/permutation step which creates a set of entities for MultiHeadedAttention processing.  This transformation and depiction below are similar to [\"Relational Deep Reinforcement Learning\"](https://arxiv.org/abs/1806.01830).\n",
    "\n",
    "\n",
    "<img src=\"images/image_to_entities.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToEntities(nn.Module):\n",
    "    def __init__(self,args,):\n",
    "        super(ImageToEntities, self).__init__()\n",
    "        self.x_dim=args.x_dim\n",
    "        self.y_dim=args.y_dim\n",
    "        self.out_channels=args.out_channels\n",
    "        self.in_channels=args.in_channels\n",
    "        self.add_output_coords=args.add_output_coords\n",
    "        self.dropout = nn.Dropout(args.conv_dropout)\n",
    "        \n",
    "        if self.add_output_coords:\n",
    "            self.out_channels-=2\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=12, stride=6, padding=0)\n",
    "        \n",
    "        if self.add_output_coords:\n",
    "            #could make this dynamic rather than hard-coding 5 by passing tmp value to convnet and testing len of features\n",
    "            self.coord_output_adder = AddCoordsTh(x_dim=5, y_dim=5, with_r=args.with_r)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.add_output_coords:\n",
    "            x = self.coord_output_adder(x)\n",
    "        x1=self.make_entities(x)\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = F.relu(x1)\n",
    "        return x2\n",
    " \n",
    "    \n",
    "    def make_entities(self,x):\n",
    "        x=x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        x=x.permute(0,2,1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AddCoordsTh(nn.Module):\n",
    "    def __init__(self, x_dim=64, y_dim=64, with_r=False):\n",
    "        super(AddCoordsTh, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor: (batch, c, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size_tensor = input_tensor.shape[0]\n",
    "\n",
    "        xx_ones = torch.ones([1, self.y_dim], dtype=torch.float32)\n",
    "        xx_ones = xx_ones.unsqueeze(-1)\n",
    "\n",
    "        xx_range = torch.arange(self.x_dim, dtype=torch.float32).unsqueeze(0)\n",
    "        xx_range = xx_range.unsqueeze(1)\n",
    "\n",
    "        xx_channel = torch.matmul(xx_ones, xx_range)\n",
    "        xx_channel = xx_channel.unsqueeze(-1)\n",
    "\n",
    "        yy_ones = torch.ones([1, self.x_dim], dtype=torch.float32)\n",
    "        yy_ones = yy_ones.unsqueeze(1)\n",
    "\n",
    "        yy_range = torch.arange(self.y_dim, dtype=torch.float32).unsqueeze(0)\n",
    "        yy_range = yy_range.unsqueeze(-1)\n",
    "\n",
    "        yy_channel = torch.matmul(yy_range, yy_ones)\n",
    "        yy_channel = yy_channel.unsqueeze(-1)\n",
    "        \n",
    "        xx_channel = xx_channel.permute(0, 3, 2, 1)\n",
    "        yy_channel = yy_channel.permute(0, 3, 2, 1)\n",
    "\n",
    "        #import pdb; pdb.set_trace()\n",
    "        xx_channel = xx_channel.float() / (self.x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (self.y_dim - 1)\n",
    "\n",
    "        #xx_channel = xx_channel * 2 - 1\n",
    "        #yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size_tensor, 1, 1, 1)\n",
    "        yy_channel = yy_channel.repeat(batch_size_tensor, 1, 1, 1)\n",
    "\n",
    "        ret = torch.cat([input_tensor, xx_channel, yy_channel], dim=1)\n",
    "        \n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params\n",
    "\n",
    "Using argparse, to ease port to command-line version.   See hack at end of main to make this work with Jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser(description='train')\n",
    "        self.add_arguments()\n",
    "        self.args = self.parser.parse_args()\n",
    "\n",
    "    def add_arguments(self): \n",
    "        self.add_conv_arguments()\n",
    "        self.add_dataset_arguments()\n",
    "        self.add_training_arguments()\n",
    "        self.add_encoder_arguments()\n",
    "        \n",
    "    def add_encoder_arguments(self):\n",
    "        # Normally encoder-count equals number of tasks in data \n",
    "        # But could be extra dormant ones to maintain model architecture across model save/restore\n",
    "        # with different datasets\n",
    "        self.parser.add_argument('--encoder-count', type=int,metavar='N',\n",
    "                                 default=3,help='number of parallel encoders')\n",
    "        self.parser.add_argument('--tasks-in-data', nargs='+', default=['0','1','2'])     \n",
    "        self.parser.add_argument('--entity_count', default=25, type=int, metavar='N',\n",
    "            help='number of entities')\n",
    "        self.parser.add_argument('--use-max',default=False,action='store_true',help=\"use torch.max rather than AggregatingMHAAttention for encoder output\")\n",
    "        self.parser.add_argument('--dropout', type=float, metavar='D',\n",
    "                            default=0.1,help='dropout probability')    \n",
    "        self.parser.add_argument('--d-model', type=int, metavar='N',\n",
    "                            default=32,help='encoder d_model')\n",
    "        self.parser.add_argument('--encoder-layers', type=int, metavar='N',\n",
    "                            default=3,help='num encoder layers')\n",
    "        self.parser.add_argument('--encoder-attention-heads', type=int, metavar='N',\n",
    "                            default=16,help='num encoder attention heads')\n",
    "        self.parser.add_argument('--attention-dropout', type=float, metavar='D',\n",
    "                            default=0.1,help='dropout probability for attention weights')\n",
    "        self.parser.add_argument('--encoder-ffn-dim', type=int, metavar='N',\n",
    "                            default=256,help='encoder dimension for FFN') \n",
    "        self.parser.add_argument('--tgt_class_count', type=int, metavar='N',\n",
    "                            default=2,help='tgt class count') \n",
    "        self.parser.add_argument('--final-module-hidden-size', type=int, metavar='N',\n",
    "                            default=1024,help='generator_hidden_size')                \n",
    "        self.parser.add_argument('--aggregate-attention-heads', type=int, metavar='N',\n",
    "                            default=8,help='number of heads for AggregatingMHAAttention step')\n",
    "      \n",
    "                        \n",
    "    def add_training_arguments(self):\n",
    "        self.parser.add_argument('--do-restore',default=False,action='store_true',help=\"do restore\")\n",
    "        self.parser.add_argument('--do-train',default=True,action='store_false',help=\"do train\") \n",
    "        self.parser.add_argument('--freeze-lower-layers',default=False,action='store_true',help=\"freeze lower layers\")\n",
    "        self.parser.add_argument('--check-interval', type=int, metavar='N',\n",
    "                                 default=100,help='check interval')\n",
    "        self.parser.add_argument('--base-path', metavar='N',\n",
    "                                 default=\"/Users/azulay/jupyter_work/\",help='base path') \n",
    "        self.parser.add_argument('--model-path', metavar='N',\n",
    "                                 default=\"model_data/saved_parms\",help='model path') \n",
    "        self.parser.add_argument('--tensorboard-path', metavar='N',\n",
    "                                 default=\"rt_logs\",help='tensorflow path')         \n",
    "        self.parser.add_argument('--batch-size', type=int, metavar='N',\n",
    "                                 default=100,help='batch size') \n",
    "        self.parser.add_argument('--test-batch-size', type=int, metavar='N',\n",
    "                                 default=200,help='batch size') \n",
    "        self.parser.add_argument('--max-epochs', type=int, metavar='N',\n",
    "                                 default=20,help='max epochs')\n",
    "        self.parser.add_argument('--default-device', metavar='N',\n",
    "                                 default=\"cpu\",help='default device')     \n",
    "        self.parser.add_argument('--ignore-saved-counts',default=True,action='store_false',help=\"do train\") \n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_dataset_arguments(self):\n",
    "        self.parser.add_argument('--data-dir', metavar='N',\n",
    "                                 default=\"datasets/3task_col_patts_pentos/\",help='data dir')      \n",
    "        self.parser.add_argument('--test-dir', metavar='N',\n",
    "                                 default=\"datasets/3task_col_patts_stripes/\",help='valid data dir')      \n",
    "        self.parser.add_argument('--report-size', type=int, metavar='N',\n",
    "                                 default=2,help='report size')  \n",
    "        self.parser.add_argument('--report-label-offset', type=int, metavar='N',\n",
    "                                 default=0,help='report label offset') \n",
    "        self.parser.add_argument('--report-task-offset', type=int, metavar='N',\n",
    "                                 default=1,help='report task offset') \n",
    "\n",
    "        \n",
    "    def add_conv_arguments(self):\n",
    "        self.parser.add_argument('--y-dim', type=int, metavar='N',\n",
    "                                 default=36,help='size of y_dimention')\n",
    "        self.parser.add_argument('--x-dim', type=int, metavar='N',\n",
    "                                 default=36,help='size of x_dimention')\n",
    "        self.parser.add_argument('--in-channels', type=int, metavar='N',\n",
    "                                 default=3,help='# input channels')\n",
    "        self.parser.add_argument('--out-channels', type=int, metavar='N',\n",
    "                                 default=32,help='# output channels')\n",
    "        self.parser.add_argument('--kernel-size', type=int, metavar='N',\n",
    "                                 default=12,help='kernel size')\n",
    "        self.parser.add_argument('--stride', type=int, metavar='N',\n",
    "                                 default=6,help='stride')\n",
    "        self.parser.add_argument('--add-output-coords', action='store_false',\n",
    "                                 default=True,help='add X and Y coordinate channels')\n",
    "        self.parser.add_argument('--with-r', action='store_false',\n",
    "                                 default=False,help='with r')\n",
    "        self.parser.add_argument('--conv-dropout', type=float, metavar='D',\n",
    "                            default=0.0,help='dropout probability')    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    \n",
    "    def __init__(self,stack,stack_optimizer,criterion,args):\n",
    "        self.stack=stack\n",
    "        self.stack_optimizer=stack_optimizer\n",
    "        self.criterion=criterion\n",
    "        self.args=args\n",
    " \n",
    "    def train(self,input_tensor, y_tensor,task_tensor): \n",
    "        \n",
    "        if args.freeze_lower_layers:\n",
    "            self.stack.eval()\n",
    "            self.stack.generator.train()\n",
    "            for task_encoder in self.stack.task_encoders:\n",
    "                task_encoder.train()\n",
    "        else:\n",
    "            self.stack.train()\n",
    "                \n",
    "        self.stack_optimizer.optimizer.zero_grad()    \n",
    "\n",
    "        stack_output,y_tensor=self.stack(input_tensor,task_tensor,y_tensor)\n",
    "\n",
    "        loss = self.criterion(stack_output, y_tensor)\n",
    "        loss.backward()\n",
    " \n",
    "        self.stack_optimizer.step()\n",
    "        _, predicted = torch.max(stack_output, 1)\n",
    "        correct = (predicted == y_tensor).sum()\n",
    "        accuracy = 100 * correct / y_tensor.size(0)\n",
    "        return loss,accuracy\n",
    "\n",
    "    \n",
    "    def eval(self,input_tensor,y_tensor,task_tensor):\n",
    "        self.stack.eval()\n",
    "        stack_output,y_tensor=self.stack(input_tensor,task_tensor,y_tensor)\n",
    "        loss = self.criterion(stack_output, y_tensor)\n",
    "        _, predicted = torch.max(stack_output, 1)\n",
    "        correct = (predicted == y_tensor).sum()\n",
    "        accuracy = 100 * correct / y_tensor.size(0)\n",
    "        return loss,accuracy\n",
    "    \n",
    "    def restore(self,model_dir):\n",
    "        checkpoint = torch.load(model_dir)\n",
    "        self.stack.load_state_dict(checkpoint['stack_state_dict'])\n",
    "        self.stack_optimizer.optimizer.load_state_dict(checkpoint['stack_optimizer_state_dict'])\n",
    "        lowest_mean_loss=checkpoint['loss']\n",
    "        lowest_test_loss=checkpoint['test_loss']\n",
    "        epoch=checkpoint['epoch']\n",
    "        batch_count=checkpoint['batch_count']\n",
    "        print(\"restoring\")\n",
    "        print(\"saved epoch was \",epoch,\"batch count was\",batch_count)\n",
    "        return lowest_mean_loss,lowest_test_loss,epoch,batch_count\n",
    "        \n",
    "\n",
    "    def save(self,mean_loss,test_loss,epoch,batch_count,model_dir):\n",
    "        torch.save({\n",
    "            'stack_state_dict': self.stack.state_dict(),\n",
    "            'stack_optimizer_state_dict': self.stack_optimizer.optimizer.state_dict(),                         \n",
    "            'loss': mean_loss,\n",
    "            'test_loss' : test_loss,\n",
    "            'epoch' : epoch,\n",
    "            'batch_count' : batch_count,\n",
    "             }, model_dir)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/azulay/opt/anaconda3/envs/new/lib/python3.6/site-packages/ipykernel_launcher.py\n",
      "Namespace(add_output_coords=True, aggregate_attention_heads=8, attention_dropout=0.1, base_path='/Users/azulay/jupyter_work/', batch_size=100, check_interval=100, conv_dropout=0.0, d_model=32, data_dir='datasets/3task_col_patts_pentos/', default_device='cpu', do_restore=False, do_train=True, dropout=0.1, encoder_attention_heads=16, encoder_count=3, encoder_ffn_dim=256, encoder_layers=3, entity_count=25, final_module_hidden_size=1024, freeze_lower_layers=False, ignore_saved_counts=True, in_channels=3, kernel_size=12, max_epochs=20, model_path='model_data/saved_parms', out_channels=32, report_label_offset=0, report_size=2, report_task_offset=1, stride=6, tasks_in_data=['0', '1', '2'], tensorboard_path='rt_logs', test_batch_size=200, test_dir='datasets/3task_col_patts_stripes/', tgt_class_count=2, use_max=False, with_r=False, x_dim=36, y_dim=36)\n",
      "got it /Users/azulay/jupyter_work/external_code\n",
      "batches per epoch: 2500\n",
      "epoch 0 count  100 loss 0.691988565325737 lowest loss 999999 accuracy 52\n",
      "epoch 0 test loss 0.696519672870636 lowest test loss 999999 test accuracy tensor(45)\n",
      "epoch 0 count  200 loss 0.5699720108509063 lowest loss 0.691988565325737 accuracy 71\n",
      "epoch 0 test loss 0.7822266221046448 lowest test loss 0.696519672870636 test accuracy tensor(58)\n",
      "epoch 0 count  300 loss 0.30574492052197455 lowest loss 0.5699720108509063 accuracy 87\n",
      "epoch 0 test loss 0.8359085321426392 lowest test loss 0.696519672870636 test accuracy tensor(64)\n",
      "epoch 0 count  400 loss 0.2092693231254816 lowest loss 0.30574492052197455 accuracy 91\n",
      "epoch 0 test loss 0.7126680016517639 lowest test loss 0.696519672870636 test accuracy tensor(69)\n",
      "epoch 0 count  500 loss 0.18677741751074792 lowest loss 0.2092693231254816 accuracy 93\n",
      "epoch 0 test loss 0.9123944044113159 lowest test loss 0.696519672870636 test accuracy tensor(65)\n",
      "epoch 0 count  600 loss 0.1468745082989335 lowest loss 0.18677741751074792 accuracy 94\n",
      "epoch 0 test loss 0.593606173992157 lowest test loss 0.696519672870636 test accuracy tensor(70)\n",
      "epoch 0 count  700 loss 0.1436185661330819 lowest loss 0.1468745082989335 accuracy 94\n",
      "epoch 0 test loss 0.4332868456840515 lowest test loss 0.593606173992157 test accuracy tensor(81)\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "\n",
    "    tensorboard_path=os.path.join(args.base_path,args.tensorboard_path)\n",
    "    data_dir=os.path.join(args.base_path,args.data_dir)\n",
    "    test_dir=os.path.join(args.base_path,args.test_dir) \n",
    "    model_path=os.path.join(args.base_path,args.model_path)\n",
    "    external_code_path=os.path.join(args.base_path,\"external_code\")\n",
    "    \n",
    "    # Dataset has to live outside this notebook in order to use set_start_method('spawn') in main\n",
    "    # Otherwise notebook hangs when using GPU.  Not sure why.  Will investigate at some point.\n",
    "    if external_code_path not in sys.path:\n",
    "        sys.path.append(external_code_path)\n",
    "        print(\"got it\",external_code_path)\n",
    "    from multi_task_dataset import MultiTaskDataset\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"thing grid\",log_dir=tensorboard_path)\n",
    "\n",
    "    lowest_mean_loss=999999\n",
    "    lowest_test_loss=999999\n",
    "    total_loss=0   \n",
    "    total_accuracy=0\n",
    "    epoch=0 \n",
    "    batch_count=0\n",
    "    check_count=0\n",
    "    data_files = os.listdir(data_dir) \n",
    "    \n",
    "    total_data_files = len(data_files)\n",
    "    batches_per_epoch=total_data_files//args.batch_size\n",
    "    print(\"batches per epoch:\",batches_per_epoch)\n",
    "\n",
    "    if args.default_device==\"cuda\":\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    stack=BranchingImageProcessor.make_model(args)\n",
    "\n",
    "    if args.default_device==\"cuda\":\n",
    "        stack=stack.cuda()\n",
    "    \n",
    "    stack_optimizer=NoamOpt.get_std_opt(stack)\n",
    "    criterion = nn.CrossEntropyLoss()   \n",
    "\n",
    "    train=Train(stack,stack_optimizer,criterion,args)\n",
    "    \n",
    "    if args.do_train==False:\n",
    "        lowest_mean_loss,lowest_test_loss,epoch=train.restore(model_path)\n",
    "        test_dataset = MultiTaskDataset(test_dir)\n",
    "            \n",
    "        test_dataloader = data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n",
    "                                shuffle=True,num_workers=1)    \n",
    "        \n",
    "        for v_batch, sample_v_batched in enumerate(test_dataloader):\n",
    "            v_image = sample_v_batched['image'].to(torch.device(args.default_device))\n",
    "            v_label = sample_v_batched['label'].to(torch.device(args.default_device)) \n",
    "            v_task = sample_v_batched['task'].to(torch.device(args.default_device))\n",
    "            test_loss,test_accuracy=train.eval(v_image,v_label,v_task)\n",
    "            print(\"Test loss\",test_loss.item(),\"test accuracy\",test_accuracy.item())\n",
    "        \n",
    "    else:\n",
    "\n",
    "        if args.do_restore:\n",
    "            lowest_mean_loss,lowest_test_loss,epoch,batch_count=train.restore(model_path)\n",
    "            if args.ignore_saved_counts==True:\n",
    "                print(\"zeroing saved counts\")\n",
    "                lowest_mean_loss=999999\n",
    "                lowest_test_loss=999999\n",
    "                epoch=0 \n",
    "                batch_count=0\n",
    "\n",
    "        if args.freeze_lower_layers:\n",
    "            print(\"!!***** FREEZING PARMS *********!!\")\n",
    "            for param in stack.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in stack.generator.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for epoch in range(epoch,args.max_epochs):\n",
    "\n",
    "            train_dataset = MultiTaskDataset(args,data_dir)\n",
    "            test_dataset = MultiTaskDataset(args,test_dir)\n",
    "\n",
    "            dataloader = data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                    shuffle=True,num_workers=1)\n",
    "\n",
    "            test_dataloader = data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n",
    "                                    shuffle=True,num_workers=1)\n",
    " \n",
    "            for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "                batch_count+=1\n",
    "                image = sample_batched['image'].to(torch.device(args.default_device))\n",
    "                label = sample_batched['label'].to(torch.device(args.default_device))\n",
    "                task = sample_batched['task'].to(torch.device(args.default_device))\n",
    "\n",
    "                loss,accuracy=train.train(image,label,task)\n",
    "                check_count+=1\n",
    "                total_loss+=loss.item()\n",
    "                total_accuracy+=accuracy.item()\n",
    "                \n",
    "                if batch_count % args.check_interval==0:\n",
    "                    mean_loss=total_loss/check_count\n",
    "                    mean_accuracy=total_accuracy//check_count\n",
    "                    writer.add_scalar(\"loss\", mean_loss, batch_count)     \n",
    "                    writer.add_scalar(\"accuracy\",accuracy,batch_count)     \n",
    "                    print(\"epoch\",epoch,\"count \",batch_count,\"loss\",mean_loss, \"lowest loss\",lowest_mean_loss,\"accuracy\",mean_accuracy)\n",
    "                    sample_v_batched = iter(test_dataloader).next()\n",
    "                    v_image = sample_v_batched['image'].to(torch.device(args.default_device))\n",
    "                    v_label = sample_v_batched['label'].to(torch.device(args.default_device)) \n",
    "                    v_task = sample_v_batched['task'].to(torch.device(args.default_device))\n",
    "                    test_loss,test_accuracy=train.eval(v_image,v_label,v_task)\n",
    "                    test_loss=test_loss.item()\n",
    "                    writer.add_scalar(\"test loss\", test_loss, batch_count)\n",
    "                    writer.add_scalar(\"test accuracy\",test_accuracy,batch_count)\n",
    "                    print(\"epoch\",epoch,\"test loss\",test_loss, \"lowest test loss\",lowest_test_loss,\"test accuracy\",test_accuracy)\n",
    "                    if test_loss<lowest_test_loss:\n",
    "                        lowest_test_loss=test_loss\n",
    "                        train.save(lowest_mean_loss,lowest_test_loss,epoch,batch_count,model_path)\n",
    "                    if mean_loss<lowest_mean_loss:\n",
    "                        lowest_mean_loss=mean_loss\n",
    "                    total_loss=0\n",
    "                    total_accuracy=0\n",
    "                    check_count=0\n",
    "                if batch_count % batches_per_epoch ==0:\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    try:\n",
    "        set_start_method('spawn',force=True)\n",
    "    except RuntimeError:\n",
    "        print(\"bad\")\n",
    "\n",
    "    \n",
    "    print(sys.argv[0])\n",
    "    sys.argv=[sys.argv[0]]\n",
    "    arguments=Arguments()\n",
    "    args=arguments.args\n",
    "    print(args)\n",
    "    main(args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
